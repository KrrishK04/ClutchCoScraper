import json

# --- Configuration ---
# The name of the file generated by your scraper
INPUT_FILE = 'clutch_listings_data_raw.jsonl'
# The name of the new file where the clean, unique data will be saved
OUTPUT_FILE = 'clutch_listings_data_unique.jsonl'

def deduplicate_jsonl(input_path, output_path):
    """
    Reads a JSONL file, removes duplicates based on the 'profile_url' key,
    and writes the unique entries to a new JSONL file.
    """
    # A set is used for highly efficient checking of whether we've seen a URL before
    seen_profile_urls = set()
    unique_entries_count = 0
    total_lines_read = 0

    print(f"Starting de-duplication process for '{input_path}'...")

    try:
        with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:
            for line in infile:
                total_lines_read += 1
                try:
                    # Convert the JSON string from the line into a Python dictionary
                    company_data = json.loads(line)
                    
                    # Get the profile_url to use as the unique identifier
                    profile_url = company_data.get('profile_url')

                    # If the URL is valid and we haven't seen it before, save it
                    if profile_url and profile_url not in seen_profile_urls:
                        # Add the URL to our set of seen URLs
                        seen_profile_urls.add(profile_url)
                        
                        # Write the original JSON line to the new file
                        outfile.write(line)
                        unique_entries_count += 1

                except json.JSONDecodeError:
                    print(f"Warning: Could not decode line {total_lines_read}. Skipping: {line.strip()}")
                    continue
    
        print("\nDe-duplication complete!")
        print(f"  - Total lines read: {total_lines_read}")
        print(f"  - Unique companies found: {unique_entries_count}")
        print(f"  - Clean data saved to: '{output_path}'")

    except FileNotFoundError:
        print(f"Error: The input file '{input_path}' was not found.")
        print("Please make sure the raw data file is in the same directory as this script.")

if __name__ == '__main__':
    deduplicate_jsonl(INPUT_FILE, OUTPUT_FILE)